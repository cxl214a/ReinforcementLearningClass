{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gomoku_main_simple.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "d29NqYiufiWo",
        "colab_type": "code",
        "outputId": "ad0f2872-620a-4ad3-aac0-808624506711",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Download libraries\n",
        "libraries = ['game.py', 'human_play.py', 'mcts_alphaZero.py', 'mcts_pure.py', 'policy_value_net.py', \n",
        "             'policy_value_net_keras.py', 'policy_value_net_numpy.py', 'policy_value_net_pytorch.py',\n",
        "             'policy_value_net_tensorflow.py']\n",
        "library_url = 'https://raw.githubusercontent.com/abx67/AlphaZero_Gomoku_my/master/simple_version/'\n",
        "\n",
        "for lib in libraries:\n",
        "  lib_url = library_url + lib\n",
        "  if not os.path.exists(lib):\n",
        "    !curl -O $lib_url"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  8040  100  8040    0     0   1427      0  0:00:05  0:00:05 --:--:--  1951\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  2889  100  2889    0     0   4086      0 --:--:-- --:--:-- --:--:--  4086\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  7844  100  7844    0     0  81708      0 --:--:-- --:--:-- --:--:-- 81708\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  7192  100  7192    0     0  78173      0 --:--:-- --:--:-- --:--:-- 78173\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  5125  100  5125    0     0  78846      0 --:--:-- --:--:-- --:--:-- 78846\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  4895  100  4895    0     0  40791      0 --:--:-- --:--:-- --:--:-- 40791\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  4037  100  4037    0     0  61166      0 --:--:-- --:--:-- --:--:-- 61166\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  6163  100  6163    0     0  94815      0 --:--:-- --:--:-- --:--:-- 94815\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  6680  100  6680    0     0  63018      0 --:--:-- --:--:-- --:--:-- 63018\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tvSzI5migHbJ",
        "colab_type": "code",
        "outputId": "07ed65d7-cb8d-4616-cea4-1f4c23c430ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install lasagne\n",
        "# !pip install --upgrade https://github.com/Lasagne/Lasagne/archive/master.zip\n",
        "# !pip install pydot==1.0.2 --upgrade"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting lasagne\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/bf/4b2336e4dbc8c8859c4dd81b1cff18eef2066b4973a1bd2b0ca2e5435f35/Lasagne-0.1.tar.gz (125kB)\n",
            "\u001b[K    100% |████████████████████████████████| 133kB 3.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from lasagne) (1.14.6)\n",
            "Building wheels for collected packages: lasagne\n",
            "  Running setup.py bdist_wheel for lasagne ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/a5/8e/31/b4cae7e5507f8582e77d7f5cf2815be8820ccacfa0519ca60c\n",
            "Successfully built lasagne\n",
            "Installing collected packages: lasagne\n",
            "Successfully installed lasagne-0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VCEx_86kmaDP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "################# ZIP AND UPLOAD FOLDER TO GOOGLE DRIVE ########################\n",
        "\n",
        "!pip install -U -q PyDrive\n",
        "\n",
        "from google.colab import files\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import zipfile\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y6iK9D2efOdf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "An implementation of the training pipeline of AlphaZero for Gomoku\n",
        "\n",
        "@author: Junxiao Song\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import print_function\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import defaultdict, deque\n",
        "from game import Board, Game\n",
        "from mcts_pure import MCTSPlayer as MCTS_Pure\n",
        "from mcts_alphaZero import MCTSPlayer\n",
        "# from policy_value_net import PolicyValueNet  # Theano and Lasagne\n",
        "# from policy_value_net_pytorch import PolicyValueNet  # Pytorch\n",
        "# from policy_value_net_tensorflow import PolicyValueNet # Tensorflow\n",
        "from policy_value_net_keras import PolicyValueNet # Keras\n",
        "\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0yWT1QMFeDWF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class TrainPipeline():\n",
        "    def __init__(self, init_model=None):\n",
        "        # params of the board and the game\n",
        "        self.bash_output = ''\n",
        "        self.f = open(\"output8by8_simple.txt\",\"w+\")\n",
        "        self.time_now = time.time()\n",
        "        \n",
        "        self.board_width = 6 \n",
        "        self.board_height = 6\n",
        "        self.n_in_row = 4\n",
        "        self.board = Board(width=self.board_width,\n",
        "                           height=self.board_height,\n",
        "                           n_in_row=self.n_in_row)\n",
        "        self.game = Game(self.board)\n",
        "        # training params\n",
        "        self.learn_rate = 2e-3\n",
        "        self.lr_multiplier = 1.0  # adaptively adjust the learning rate based on KL\n",
        "        self.temp = 1.0  # the temperature param\n",
        "        self.n_playout = 400  # num of simulations for each move\n",
        "        self.c_puct = 5\n",
        "        self.buffer_size = 10000\n",
        "        self.batch_size = 512  # mini-batch size for training\n",
        "        self.data_buffer = deque(maxlen=self.buffer_size)\n",
        "        self.play_batch_size = 1\n",
        "        self.epochs = 5  # num of train_steps for each update\n",
        "        self.kl_targ = 0.02\n",
        "        self.check_freq = 200\n",
        "        self.game_batch_num = 2000\n",
        "        self.best_win_ratio = 0.0\n",
        "        # num of simulations used for the pure mcts, which is used as\n",
        "        # the opponent to evaluate the trained policy\n",
        "        self.pure_mcts_playout_num = 1000\n",
        "        if init_model:\n",
        "            # start training from an initial policy-value net\n",
        "            self.policy_value_net = PolicyValueNet(self.board_width,\n",
        "                                                   self.board_height,\n",
        "                                                   model_file=init_model)\n",
        "        else:\n",
        "            # start training from a new policy-value net\n",
        "            self.policy_value_net = PolicyValueNet(self.board_width,\n",
        "                                                   self.board_height)\n",
        "        self.mcts_player = MCTSPlayer(self.policy_value_net.policy_value_fn,\n",
        "                                      c_puct=self.c_puct,\n",
        "                                      n_playout=self.n_playout,\n",
        "                                      is_selfplay=1)\n",
        "\n",
        "    def get_equi_data(self, play_data):\n",
        "        \"\"\"augment the data set by rotation and flipping\n",
        "        play_data: [(state, mcts_prob, winner_z), ..., ...]\n",
        "        \"\"\"\n",
        "        extend_data = []\n",
        "        for state, mcts_porb, winner in play_data:\n",
        "            for i in [1, 2, 3, 4]:\n",
        "                # rotate counterclockwise\n",
        "                equi_state = np.array([np.rot90(s, i) for s in state])\n",
        "                equi_mcts_prob = np.rot90(np.flipud(\n",
        "                    mcts_porb.reshape(self.board_height, self.board_width)), i)\n",
        "                extend_data.append((equi_state,\n",
        "                                    np.flipud(equi_mcts_prob).flatten(),\n",
        "                                    winner))\n",
        "                # flip horizontally\n",
        "                equi_state = np.array([np.fliplr(s) for s in equi_state])\n",
        "                equi_mcts_prob = np.fliplr(equi_mcts_prob)\n",
        "                extend_data.append((equi_state,\n",
        "                                    np.flipud(equi_mcts_prob).flatten(),\n",
        "                                    winner))\n",
        "        return extend_data\n",
        "\n",
        "    def collect_selfplay_data(self, n_games=1):\n",
        "        \"\"\"collect self-play data for training\"\"\"\n",
        "        for i in range(n_games):\n",
        "            winner, play_data = self.game.start_self_play(self.mcts_player,\n",
        "                                                          temp=self.temp)\n",
        "            print(play_data)  # fanerror\n",
        "            play_data = list(play_data)[:]\n",
        "            self.episode_len = len(play_data)\n",
        "            # augment the data\n",
        "            play_data = self.get_equi_data(play_data)\n",
        "            self.data_buffer.extend(play_data)\n",
        "\n",
        "    def policy_update(self):\n",
        "        \"\"\"update the policy-value net\"\"\"\n",
        "        mini_batch = random.sample(self.data_buffer, self.batch_size)\n",
        "        state_batch = [data[0] for data in mini_batch]\n",
        "        mcts_probs_batch = [data[1] for data in mini_batch]\n",
        "        winner_batch = [data[2] for data in mini_batch]\n",
        "        old_probs, old_v = self.policy_value_net.policy_value(state_batch)\n",
        "        for i in range(self.epochs):\n",
        "            loss, entropy = self.policy_value_net.train_step(\n",
        "                    state_batch,\n",
        "                    mcts_probs_batch,\n",
        "                    winner_batch,\n",
        "                    self.learn_rate*self.lr_multiplier)\n",
        "            new_probs, new_v = self.policy_value_net.policy_value(state_batch)\n",
        "            kl = np.mean(np.sum(old_probs * (\n",
        "                    np.log(old_probs + 1e-10) - np.log(new_probs + 1e-10)),\n",
        "                    axis=1)\n",
        "            )\n",
        "            if kl > self.kl_targ * 4:  # early stopping if D_KL diverges badly\n",
        "                break\n",
        "        # adaptively adjust the learning rate\n",
        "        if kl > self.kl_targ * 2 and self.lr_multiplier > 0.1:\n",
        "            self.lr_multiplier /= 1.5\n",
        "        elif kl < self.kl_targ / 2 and self.lr_multiplier < 10:\n",
        "            self.lr_multiplier *= 1.5\n",
        "\n",
        "        explained_var_old = (1 -\n",
        "                             np.var(np.array(winner_batch) - old_v.flatten()) /\n",
        "                             np.var(np.array(winner_batch)))\n",
        "        explained_var_new = (1 -\n",
        "                             np.var(np.array(winner_batch) - new_v.flatten()) /\n",
        "                             np.var(np.array(winner_batch)))\n",
        "        print((\"kl:{:.5f},\"\n",
        "               \"lr_multiplier:{:.3f},\"\n",
        "               \"loss:{},\"\n",
        "               \"entropy:{},\"\n",
        "               \"explained_var_old:{:.3f},\"\n",
        "               \"explained_var_new:{:.3f}\"\n",
        "               ).format(kl,\n",
        "                        self.lr_multiplier,\n",
        "                        loss,\n",
        "                        entropy,\n",
        "                        explained_var_old,\n",
        "                        explained_var_new))\n",
        "        \n",
        "        self.bash_output = (\"kl:{:.5f},\"\n",
        "                           \"lr_multiplier:{:.3f},\"\n",
        "                           \"loss:{},\"\n",
        "                           \"entropy:{},\"\n",
        "                           \"explained_var_old:{:.3f},\"\n",
        "                           \"explained_var_new:{:.3f}\"\n",
        "                           ).format(kl,\n",
        "                            self.lr_multiplier,\n",
        "                            loss,\n",
        "                            entropy,\n",
        "                            explained_var_old,\n",
        "                            explained_var_new)\n",
        "        self.f.write(self.bash_output)\n",
        "        self.f.write('\\n')\n",
        "        self.bash_output = ''\n",
        "        \n",
        "        return loss, entropy\n",
        "\n",
        "    def policy_evaluate(self, n_games=10):\n",
        "        \"\"\"\n",
        "        Evaluate the trained policy by playing against the pure MCTS player\n",
        "        Note: this is only for monitoring the progress of training\n",
        "        \"\"\"\n",
        "        current_mcts_player = MCTSPlayer(self.policy_value_net.policy_value_fn,\n",
        "                                         c_puct=self.c_puct,\n",
        "                                         n_playout=self.n_playout)\n",
        "        pure_mcts_player = MCTS_Pure(c_puct=5,\n",
        "                                     n_playout=self.pure_mcts_playout_num)\n",
        "        win_cnt = defaultdict(int)\n",
        "        for i in range(n_games):\n",
        "            winner = self.game.start_play(current_mcts_player,\n",
        "                                          pure_mcts_player,\n",
        "                                          start_player=i % 2,\n",
        "                                          is_shown=0)\n",
        "            win_cnt[winner] += 1\n",
        "        win_ratio = 1.0*(win_cnt[1] + 0.5*win_cnt[-1]) / n_games\n",
        "        print(\"num_playouts:{}, win: {}, lose: {}, tie:{}\".format(\n",
        "                self.pure_mcts_playout_num,\n",
        "                win_cnt[1], win_cnt[2], win_cnt[-1]))\n",
        "        return win_ratio\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"run the training pipeline\"\"\"\n",
        "        self.time_now = time.time()\n",
        "        start_time = time.time()\n",
        "        try:\n",
        "            for i in range(self.game_batch_num):\n",
        "              \n",
        "                print('Time elapsed: {} seconds'.format(round(time.time() - self.time_now)) + \n",
        "                      '\\t Total time elapsed: {} seconds'.format(round(time.time() - start_time)))\n",
        "                self.f.write('Time elapsed: {} seconds'.format(round(time.time() - self.time_now)) + \n",
        "                      '\\t Total time elapsed: {} seconds'.format(round(time.time() - start_time)))\n",
        "                self.f.write(\"\\n\")\n",
        "                \n",
        "                self.collect_selfplay_data(self.play_batch_size)\n",
        "                print(\"batch i:{}, episode_len:{}\".format(\n",
        "                        i+1, self.episode_len))\n",
        "                self.f.write(\"batch i:{}, episode_len:{}\".format(\n",
        "                        i+1, self.episode_len))\n",
        "                self.f.write(\"\\n\")\n",
        "                if len(self.data_buffer) > self.batch_size:\n",
        "                    loss, entropy = self.policy_update()\n",
        "                    \n",
        "                # check the performance of the current model,\n",
        "                # and save the model params\n",
        "                if (i+1) % self.check_freq == 0:\n",
        "                  \n",
        "                    self.f.close()\n",
        "                    # save the output figures in google drive\n",
        "                    auth.authenticate_user()\n",
        "                    gauth = GoogleAuth()\n",
        "                    gauth.credentials = GoogleCredentials.get_application_default()\n",
        "                    drive = GoogleDrive(gauth)\n",
        "\n",
        "                    file = drive.CreateFile()\n",
        "                    file.SetContentFile('output8by8_simple.txt')\n",
        "                    file.Upload()\n",
        "                    self.f = open(\"output8by8_simple.txt\",\"a\")\n",
        "                  \n",
        "                  \n",
        "                  \n",
        "                    print(\"current self-play batch: {}\".format(i+1))\n",
        "                    win_ratio = self.policy_evaluate()\n",
        "                    self.policy_value_net.save_model('./current_policy.model')\n",
        "                    \n",
        "                    # save the output figures in google drive\n",
        "\n",
        "                    file = drive.CreateFile()\n",
        "                    file.SetContentFile('current_policy.model')\n",
        "                    file.Upload()\n",
        "                    \n",
        "                    if win_ratio > self.best_win_ratio:\n",
        "                        print(\"New best policy!!!!!!!!\")\n",
        "                        self.best_win_ratio = win_ratio\n",
        "                        # update the best_policy\n",
        "                        self.policy_value_net.save_model('./best_policy.model')\n",
        "                        \n",
        "                        # save the output figures in google drive\n",
        "                        auth.authenticate_user()\n",
        "                        gauth = GoogleAuth()\n",
        "                        gauth.credentials = GoogleCredentials.get_application_default()\n",
        "                        drive = GoogleDrive(gauth)\n",
        "\n",
        "                        file = drive.CreateFile()\n",
        "                        file.SetContentFile('best_policy.model')\n",
        "                        file.Upload()\n",
        "                  \n",
        "                        if (self.best_win_ratio == 1.0 and\n",
        "                                self.pure_mcts_playout_num < 5000):\n",
        "                            self.pure_mcts_playout_num += 1000\n",
        "                            self.best_win_ratio = 0.0\n",
        "                  self.time_now = time.time()\n",
        "            self.f.close()\n",
        "        except KeyboardInterrupt:\n",
        "            print('\\n\\rquit')\n",
        "            self.f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D9WKEb8CfRX9",
        "colab_type": "code",
        "outputId": "63128929-3908-4940-d330-4553629e65a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    training_pipeline = TrainPipeline()\n",
        "    training_pipeline.run()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time elapsed: 0 seconds\t Total time elapsed: 0 seconds\n",
            "<zip object at 0x7fbd7eadc9c8>\n",
            "batch i:1, episode_len:16\n",
            "Time elapsed: 17 seconds\t Total time elapsed: 18 seconds\n",
            "<zip object at 0x7fbd7ed54f08>\n",
            "batch i:2, episode_len:12\n",
            "Time elapsed: 13 seconds\t Total time elapsed: 31 seconds\n",
            "<zip object at 0x7fbd8033af48>\n",
            "batch i:3, episode_len:13\n",
            "Time elapsed: 14 seconds\t Total time elapsed: 46 seconds\n",
            "<zip object at 0x7fbd7f9f5a08>\n",
            "batch i:4, episode_len:18\n",
            "Time elapsed: 19 seconds\t Total time elapsed: 66 seconds\n",
            "<zip object at 0x7fbd7f804148>\n",
            "batch i:5, episode_len:12\n",
            "kl:0.00079,lr_multiplier:1.500,loss:4.311645030975342,entropy:3.583465576171875,explained_var_old:-0.008,explained_var_new:0.384\n",
            "Time elapsed: 28 seconds\t Total time elapsed: 94 seconds\n",
            "<zip object at 0x7fbd7f804d88>\n",
            "batch i:6, episode_len:13\n",
            "kl:0.00047,lr_multiplier:2.250,loss:4.408133029937744,entropy:3.5830283164978027,explained_var_old:0.205,explained_var_new:0.268\n",
            "Time elapsed: 27 seconds\t Total time elapsed: 122 seconds\n",
            "<zip object at 0x7fbd7e3a9508>\n",
            "batch i:7, episode_len:21\n",
            "\n",
            "quit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8jOSn_Uhhnxh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
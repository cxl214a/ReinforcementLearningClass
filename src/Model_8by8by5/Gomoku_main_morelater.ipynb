{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gomoku_main_morelayer.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "zY4O3DRUBK1H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 5 layers"
      ]
    },
    {
      "metadata": {
        "id": "d29NqYiufiWo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Download libraries\n",
        "libraries = ['game.py', 'human_play.py', 'mcts_alphaZero.py', 'mcts_pure.py', 'policy_value_net.py', \n",
        "             'policy_value_net_keras.py', 'policy_value_net_numpy.py', 'policy_value_net_pytorch.py',\n",
        "             'policy_value_net_tensorflow.py']\n",
        "library_url = 'https://raw.githubusercontent.com/abx67/AlphaZero_Gomoku_my/master/morelayer_version/'\n",
        "\n",
        "for lib in libraries:\n",
        "  lib_url = library_url + lib\n",
        "  if not os.path.exists(lib):\n",
        "    !curl -O $lib_url"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tvSzI5migHbJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install lasagne\n",
        "# !pip install --upgrade https://github.com/Lasagne/Lasagne/archive/master.zip\n",
        "# !pip install pydot==1.0.2 --upgrade"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VCEx_86kmaDP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "################# ZIP AND UPLOAD FOLDER TO GOOGLE DRIVE ########################\n",
        "\n",
        "!pip install -U -q PyDrive\n",
        "\n",
        "from google.colab import files\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import zipfile\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y6iK9D2efOdf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7a6549e0-e9fb-4323-f13e-9b910f53c419"
      },
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "An implementation of the training pipeline of AlphaZero for Gomoku\n",
        "\n",
        "@author: Junxiao Song\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import print_function\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import defaultdict, deque\n",
        "from game import Board, Game\n",
        "from mcts_pure import MCTSPlayer as MCTS_Pure\n",
        "from mcts_alphaZero import MCTSPlayer\n",
        "# from policy_value_net import PolicyValueNet  # Theano and Lasagne\n",
        "# from policy_value_net_pytorch import PolicyValueNet  # Pytorch\n",
        "# from policy_value_net_tensorflow import PolicyValueNet # Tensorflow\n",
        "from policy_value_net_keras import PolicyValueNet # Keras\n",
        "\n",
        "import time"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "0yWT1QMFeDWF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class TrainPipeline():\n",
        "    def __init__(self, init_model=None):\n",
        "        # params of the board and the game\n",
        "        self.bash_output = ''\n",
        "        self.f = open(\"output8by8_5layerslayer.txt\",\"w+\")\n",
        "        self.time_now = time.time()\n",
        "        \n",
        "        self.board_width = 8 \n",
        "        self.board_height = 8\n",
        "        self.n_in_row = 5\n",
        "        self.board = Board(width=self.board_width,\n",
        "                           height=self.board_height,\n",
        "                           n_in_row=self.n_in_row)\n",
        "        self.game = Game(self.board)\n",
        "        # training params\n",
        "        self.learn_rate = 2e-3\n",
        "        self.lr_multiplier = 1.0  # adaptively adjust the learning rate based on KL\n",
        "        self.temp = 1.0  # the temperature param\n",
        "        self.n_playout = 400  # num of simulations for each move\n",
        "        self.c_puct = 5\n",
        "        self.buffer_size = 10000\n",
        "        self.batch_size = 512  # mini-batch size for training\n",
        "        self.data_buffer = deque(maxlen=self.buffer_size)\n",
        "        self.play_batch_size = 1\n",
        "        self.epochs = 5  # num of train_steps for each update\n",
        "        self.kl_targ = 0.02\n",
        "        self.check_freq = 200\n",
        "        self.game_batch_num = 2000\n",
        "        self.best_win_ratio = 0.0\n",
        "        # num of simulations used for the pure mcts, which is used as\n",
        "        # the opponent to evaluate the trained policy\n",
        "        self.pure_mcts_playout_num = 1000\n",
        "        if init_model:\n",
        "            # start training from an initial policy-value net\n",
        "            self.policy_value_net = PolicyValueNet(self.board_width,\n",
        "                                                   self.board_height,\n",
        "                                                   model_file=init_model)\n",
        "        else:\n",
        "            # start training from a new policy-value net\n",
        "            self.policy_value_net = PolicyValueNet(self.board_width,\n",
        "                                                   self.board_height)\n",
        "        self.mcts_player = MCTSPlayer(self.policy_value_net.policy_value_fn,\n",
        "                                      c_puct=self.c_puct,\n",
        "                                      n_playout=self.n_playout,\n",
        "                                      is_selfplay=1)\n",
        "\n",
        "    def get_equi_data(self, play_data):\n",
        "        \"\"\"augment the data set by rotation and flipping\n",
        "        play_data: [(state, mcts_prob, winner_z), ..., ...]\n",
        "        \"\"\"\n",
        "        extend_data = []\n",
        "        for state, mcts_porb, winner in play_data:\n",
        "            for i in [1, 2, 3, 4]:\n",
        "                # rotate counterclockwise\n",
        "                equi_state = np.array([np.rot90(s, i) for s in state])\n",
        "                equi_mcts_prob = np.rot90(np.flipud(\n",
        "                    mcts_porb.reshape(self.board_height, self.board_width)), i)\n",
        "                extend_data.append((equi_state,\n",
        "                                    np.flipud(equi_mcts_prob).flatten(),\n",
        "                                    winner))\n",
        "                # flip horizontally\n",
        "                equi_state = np.array([np.fliplr(s) for s in equi_state])\n",
        "                equi_mcts_prob = np.fliplr(equi_mcts_prob)\n",
        "                extend_data.append((equi_state,\n",
        "                                    np.flipud(equi_mcts_prob).flatten(),\n",
        "                                    winner))\n",
        "        return extend_data\n",
        "\n",
        "    def collect_selfplay_data(self, n_games=1):\n",
        "        \"\"\"collect self-play data for training\"\"\"\n",
        "        for i in range(n_games):\n",
        "            winner, play_data = self.game.start_self_play(self.mcts_player,\n",
        "                                                          temp=self.temp)\n",
        "            print(play_data)  # fanerror\n",
        "            play_data = list(play_data)[:]\n",
        "            self.episode_len = len(play_data)\n",
        "            # augment the data\n",
        "            play_data = self.get_equi_data(play_data)\n",
        "            self.data_buffer.extend(play_data)\n",
        "\n",
        "    def policy_update(self):\n",
        "        \"\"\"update the policy-value net\"\"\"\n",
        "        mini_batch = random.sample(self.data_buffer, self.batch_size)\n",
        "        state_batch = [data[0] for data in mini_batch]\n",
        "        mcts_probs_batch = [data[1] for data in mini_batch]\n",
        "        winner_batch = [data[2] for data in mini_batch]\n",
        "        old_probs, old_v = self.policy_value_net.policy_value(state_batch)\n",
        "        for i in range(self.epochs):\n",
        "            loss, entropy = self.policy_value_net.train_step(\n",
        "                    state_batch,\n",
        "                    mcts_probs_batch,\n",
        "                    winner_batch,\n",
        "                    self.learn_rate*self.lr_multiplier)\n",
        "            new_probs, new_v = self.policy_value_net.policy_value(state_batch)\n",
        "            kl = np.mean(np.sum(old_probs * (\n",
        "                    np.log(old_probs + 1e-10) - np.log(new_probs + 1e-10)),\n",
        "                    axis=1)\n",
        "            )\n",
        "            if kl > self.kl_targ * 4:  # early stopping if D_KL diverges badly\n",
        "                break\n",
        "        # adaptively adjust the learning rate\n",
        "        if kl > self.kl_targ * 2 and self.lr_multiplier > 0.1:\n",
        "            self.lr_multiplier /= 1.5\n",
        "        elif kl < self.kl_targ / 2 and self.lr_multiplier < 10:\n",
        "            self.lr_multiplier *= 1.5\n",
        "\n",
        "        explained_var_old = (1 -\n",
        "                             np.var(np.array(winner_batch) - old_v.flatten()) /\n",
        "                             np.var(np.array(winner_batch)))\n",
        "        explained_var_new = (1 -\n",
        "                             np.var(np.array(winner_batch) - new_v.flatten()) /\n",
        "                             np.var(np.array(winner_batch)))\n",
        "        print((\"kl:{:.5f},\"\n",
        "               \"lr_multiplier:{:.3f},\"\n",
        "               \"loss:{},\"\n",
        "               \"entropy:{},\"\n",
        "               \"explained_var_old:{:.3f},\"\n",
        "               \"explained_var_new:{:.3f}\"\n",
        "               ).format(kl,\n",
        "                        self.lr_multiplier,\n",
        "                        loss,\n",
        "                        entropy,\n",
        "                        explained_var_old,\n",
        "                        explained_var_new))\n",
        "        \n",
        "        self.bash_output = (\"kl:{:.5f},\"\n",
        "                           \"lr_multiplier:{:.3f},\"\n",
        "                           \"loss:{},\"\n",
        "                           \"entropy:{},\"\n",
        "                           \"explained_var_old:{:.3f},\"\n",
        "                           \"explained_var_new:{:.3f}\"\n",
        "                           ).format(kl,\n",
        "                            self.lr_multiplier,\n",
        "                            loss,\n",
        "                            entropy,\n",
        "                            explained_var_old,\n",
        "                            explained_var_new)\n",
        "        self.f.write(self.bash_output)\n",
        "        self.f.write('\\n')\n",
        "        self.bash_output = ''\n",
        "        \n",
        "        return loss, entropy\n",
        "\n",
        "    def policy_evaluate(self, n_games=10):\n",
        "        \"\"\"\n",
        "        Evaluate the trained policy by playing against the pure MCTS player\n",
        "        Note: this is only for monitoring the progress of training\n",
        "        \"\"\"\n",
        "        current_mcts_player = MCTSPlayer(self.policy_value_net.policy_value_fn,\n",
        "                                         c_puct=self.c_puct,\n",
        "                                         n_playout=self.n_playout)\n",
        "        pure_mcts_player = MCTS_Pure(c_puct=5,\n",
        "                                     n_playout=self.pure_mcts_playout_num)\n",
        "        win_cnt = defaultdict(int)\n",
        "        for i in range(n_games):\n",
        "            winner = self.game.start_play(current_mcts_player,\n",
        "                                          pure_mcts_player,\n",
        "                                          start_player=i % 2,\n",
        "                                          is_shown=0)\n",
        "            win_cnt[winner] += 1\n",
        "        win_ratio = 1.0*(win_cnt[1] + 0.5*win_cnt[-1]) / n_games\n",
        "        print(\"num_playouts:{}, win: {}, lose: {}, tie:{}\".format(\n",
        "                self.pure_mcts_playout_num,\n",
        "                win_cnt[1], win_cnt[2], win_cnt[-1]))\n",
        "        return win_ratio\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"run the training pipeline\"\"\"\n",
        "        self.time_now = time.time()\n",
        "        start_time = time.time()\n",
        "        try:\n",
        "            for i in range(self.game_batch_num):\n",
        "              \n",
        "                print('Time elapsed: {} seconds'.format(round(time.time() - self.time_now)) + \n",
        "                      '\\t Total time elapsed: {} seconds'.format(round(time.time() - start_time)))\n",
        "                self.f.write('Time elapsed: {} seconds'.format(round(time.time() - self.time_now)) + \n",
        "                      '\\t Total time elapsed: {} seconds'.format(round(time.time() - start_time)))\n",
        "                self.f.write(\"\\n\")\n",
        "                \n",
        "                self.collect_selfplay_data(self.play_batch_size)\n",
        "                print(\"batch i:{}, episode_len:{}\".format(\n",
        "                        i+1, self.episode_len))\n",
        "                self.f.write(\"batch i:{}, episode_len:{}\".format(\n",
        "                        i+1, self.episode_len))\n",
        "                self.f.write(\"\\n\")\n",
        "                if len(self.data_buffer) > self.batch_size:\n",
        "                    loss, entropy = self.policy_update()\n",
        "                    \n",
        "                # check the performance of the current model,\n",
        "                # and save the model params\n",
        "                if (i+1) % self.check_freq == 0:\n",
        "                  \n",
        "                    self.f.close()\n",
        "                    # save the output figures in google drive\n",
        "                    auth.authenticate_user()\n",
        "                    gauth = GoogleAuth()\n",
        "                    gauth.credentials = GoogleCredentials.get_application_default()\n",
        "                    drive = GoogleDrive(gauth)\n",
        "\n",
        "                    file = drive.CreateFile()\n",
        "                    file.SetContentFile('output8by8_5layers.txt')\n",
        "                    file.Upload()\n",
        "                    self.f = open(\"output8by8_5layers.txt\",\"a\")\n",
        "                  \n",
        "                  \n",
        "                  \n",
        "                    print(\"current self-play batch: {}\".format(i+1))\n",
        "                    win_ratio = self.policy_evaluate()\n",
        "                    self.policy_value_net.save_model('./current_policy.model')\n",
        "                    \n",
        "                    # save the output figures in google drive\n",
        "\n",
        "                    file = drive.CreateFile()\n",
        "                    file.SetContentFile('current_policy.model')\n",
        "                    file.Upload()\n",
        "                    \n",
        "                    if win_ratio > self.best_win_ratio:\n",
        "                        print(\"New best policy!!!!!!!!\")\n",
        "                        self.best_win_ratio = win_ratio\n",
        "                        # update the best_policy\n",
        "                        self.policy_value_net.save_model('./best_policy.model')\n",
        "                        \n",
        "                        # save the output figures in google drive\n",
        "                        auth.authenticate_user()\n",
        "                        gauth = GoogleAuth()\n",
        "                        gauth.credentials = GoogleCredentials.get_application_default()\n",
        "                        drive = GoogleDrive(gauth)\n",
        "\n",
        "                        file = drive.CreateFile()\n",
        "                        file.SetContentFile('best_policy.model')\n",
        "                        file.Upload()\n",
        "                  \n",
        "                        if (self.best_win_ratio == 1.0 and\n",
        "                                self.pure_mcts_playout_num < 5000):\n",
        "                            self.pure_mcts_playout_num += 1000\n",
        "                            self.best_win_ratio = 0.0\n",
        "                  self.time_now = time.time()\n",
        "            self.f.close()\n",
        "        except KeyboardInterrupt:\n",
        "            print('\\n\\rquit')\n",
        "            self.f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D9WKEb8CfRX9",
        "colab_type": "code",
        "outputId": "fc51c778-4a80-40e6-e258-141727d26c87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2672
        }
      },
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    training_pipeline = TrainPipeline()\n",
        "    training_pipeline.run()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time elapsed: 0 seconds\t Total time elapsed: 0 seconds\n",
            "<zip object at 0x7f88475663c8>\n",
            "batch i:1, episode_len:27\n",
            "Time elapsed: 38 seconds\t Total time elapsed: 39 seconds\n",
            "<zip object at 0x7f88475d0a08>\n",
            "batch i:2, episode_len:36\n",
            "Time elapsed: 49 seconds\t Total time elapsed: 88 seconds\n",
            "<zip object at 0x7f884b046908>\n",
            "batch i:3, episode_len:19\n",
            "kl:0.00108,lr_multiplier:1.500,loss:5.137330532073975,entropy:4.1588544845581055,explained_var_old:-0.004,explained_var_new:0.082\n",
            "Time elapsed: 50 seconds\t Total time elapsed: 139 seconds\n",
            "<zip object at 0x7f8846734548>\n",
            "batch i:4, episode_len:38\n",
            "kl:0.00089,lr_multiplier:2.250,loss:4.894233226776123,entropy:4.158131122589111,explained_var_old:0.066,explained_var_new:0.412\n",
            "Time elapsed: 75 seconds\t Total time elapsed: 214 seconds\n",
            "<zip object at 0x7f8845eb37c8>\n",
            "batch i:5, episode_len:26\n",
            "kl:0.01138,lr_multiplier:2.250,loss:4.60254430770874,entropy:4.138716697692871,explained_var_old:0.307,explained_var_new:0.714\n",
            "Time elapsed: 59 seconds\t Total time elapsed: 275 seconds\n",
            "<zip object at 0x7f8845eb3f08>\n",
            "batch i:6, episode_len:29\n",
            "kl:0.01292,lr_multiplier:2.250,loss:4.300754547119141,entropy:4.133336067199707,explained_var_old:0.555,explained_var_new:0.881\n",
            "Time elapsed: 65 seconds\t Total time elapsed: 340 seconds\n",
            "<zip object at 0x7f8845ec4908>\n",
            "batch i:7, episode_len:24\n",
            "kl:0.01025,lr_multiplier:2.250,loss:4.2431254386901855,entropy:4.122251510620117,explained_var_old:0.658,explained_var_new:0.911\n",
            "Time elapsed: 58 seconds\t Total time elapsed: 399 seconds\n",
            "<zip object at 0x7f884b074548>\n",
            "batch i:8, episode_len:33\n",
            "kl:0.03418,lr_multiplier:2.250,loss:4.181544780731201,entropy:4.033881187438965,explained_var_old:0.818,explained_var_new:0.938\n",
            "Time elapsed: 73 seconds\t Total time elapsed: 472 seconds\n",
            "<zip object at 0x7f88474f8e48>\n",
            "batch i:9, episode_len:32\n",
            "kl:0.11818,lr_multiplier:1.500,loss:4.296454429626465,entropy:4.130692958831787,explained_var_old:0.701,explained_var_new:0.852\n",
            "Time elapsed: 64 seconds\t Total time elapsed: 536 seconds\n",
            "<zip object at 0x7f88474f8b48>\n",
            "batch i:10, episode_len:22\n",
            "kl:0.06430,lr_multiplier:1.000,loss:4.244651794433594,entropy:4.120874404907227,explained_var_old:0.689,explained_var_new:0.889\n",
            "Time elapsed: 55 seconds\t Total time elapsed: 591 seconds\n",
            "<zip object at 0x7f8846734848>\n",
            "batch i:11, episode_len:35\n",
            "kl:0.01066,lr_multiplier:1.000,loss:4.299988269805908,entropy:4.038373947143555,explained_var_old:0.596,explained_var_new:0.791\n",
            "Time elapsed: 76 seconds\t Total time elapsed: 668 seconds\n",
            "<zip object at 0x7f88465e1388>\n",
            "batch i:12, episode_len:32\n",
            "kl:0.01270,lr_multiplier:1.000,loss:4.221129894256592,entropy:4.054045677185059,explained_var_old:0.774,explained_var_new:0.859\n",
            "Time elapsed: 69 seconds\t Total time elapsed: 736 seconds\n",
            "<zip object at 0x7f88472751c8>\n",
            "batch i:13, episode_len:24\n",
            "kl:0.00751,lr_multiplier:1.500,loss:4.440363883972168,entropy:4.056334972381592,explained_var_old:0.508,explained_var_new:0.654\n",
            "Time elapsed: 57 seconds\t Total time elapsed: 794 seconds\n",
            "<zip object at 0x7f884757c048>\n",
            "batch i:14, episode_len:23\n",
            "kl:0.01862,lr_multiplier:1.500,loss:4.488198280334473,entropy:4.033082962036133,explained_var_old:0.431,explained_var_new:0.545\n",
            "Time elapsed: 57 seconds\t Total time elapsed: 852 seconds\n",
            "<zip object at 0x7f8846c65388>\n",
            "batch i:15, episode_len:30\n",
            "kl:0.02478,lr_multiplier:1.500,loss:4.543629169464111,entropy:3.9992947578430176,explained_var_old:0.263,explained_var_new:0.456\n",
            "Time elapsed: 69 seconds\t Total time elapsed: 921 seconds\n",
            "<zip object at 0x7f8881310488>\n",
            "batch i:16, episode_len:39\n",
            "kl:0.04776,lr_multiplier:1.000,loss:4.713038444519043,entropy:3.978499412536621,explained_var_old:0.078,explained_var_new:0.303\n",
            "Time elapsed: 78 seconds\t Total time elapsed: 1000 seconds\n",
            "<zip object at 0x7f8846c64888>\n",
            "batch i:17, episode_len:33\n",
            "kl:0.02763,lr_multiplier:1.000,loss:4.957326889038086,entropy:3.940493583679199,explained_var_old:-0.171,explained_var_new:-0.010\n",
            "Time elapsed: 69 seconds\t Total time elapsed: 1070 seconds\n",
            "<zip object at 0x7f8847537e88>\n",
            "batch i:18, episode_len:17\n",
            "kl:0.03485,lr_multiplier:1.000,loss:4.854798316955566,entropy:3.9510202407836914,explained_var_old:-0.029,explained_var_new:0.121\n",
            "Time elapsed: 47 seconds\t Total time elapsed: 1116 seconds\n",
            "<zip object at 0x7f88463d3048>\n",
            "batch i:19, episode_len:19\n",
            "kl:0.03262,lr_multiplier:1.000,loss:4.803181171417236,entropy:3.9176502227783203,explained_var_old:-0.042,explained_var_new:0.161\n",
            "Time elapsed: 53 seconds\t Total time elapsed: 1170 seconds\n",
            "<zip object at 0x7f88463d3348>\n",
            "batch i:20, episode_len:17\n",
            "kl:0.04394,lr_multiplier:0.667,loss:4.791584491729736,entropy:3.9029226303100586,explained_var_old:-0.071,explained_var_new:0.156\n",
            "Time elapsed: 47 seconds\t Total time elapsed: 1217 seconds\n",
            "<zip object at 0x7f8846c65548>\n",
            "batch i:21, episode_len:33\n",
            "kl:0.01983,lr_multiplier:0.667,loss:4.781732559204102,entropy:3.917678117752075,explained_var_old:-0.061,explained_var_new:0.151\n",
            "Time elapsed: 71 seconds\t Total time elapsed: 1289 seconds\n",
            "<zip object at 0x7f88465ded88>\n",
            "batch i:22, episode_len:26\n",
            "kl:0.02836,lr_multiplier:0.667,loss:4.6913580894470215,entropy:3.8724498748779297,explained_var_old:-0.012,explained_var_new:0.229\n",
            "Time elapsed: 59 seconds\t Total time elapsed: 1348 seconds\n",
            "<zip object at 0x7f8847578d88>\n",
            "batch i:23, episode_len:38\n",
            "kl:0.02099,lr_multiplier:0.667,loss:4.637547016143799,entropy:3.9129562377929688,explained_var_old:0.082,explained_var_new:0.267\n",
            "Time elapsed: 75 seconds\t Total time elapsed: 1424 seconds\n",
            "<zip object at 0x7f8845ec4448>\n",
            "batch i:24, episode_len:64\n",
            "kl:0.03997,lr_multiplier:0.667,loss:4.469265460968018,entropy:3.8533337116241455,explained_var_old:0.113,explained_var_new:0.318\n",
            "Time elapsed: 111 seconds\t Total time elapsed: 1535 seconds\n",
            "<zip object at 0x7f884c2ae588>\n",
            "batch i:25, episode_len:26\n",
            "kl:0.04765,lr_multiplier:0.444,loss:4.496631622314453,entropy:3.8820340633392334,explained_var_old:0.184,explained_var_new:0.340\n",
            "Time elapsed: 58 seconds\t Total time elapsed: 1594 seconds\n",
            "<zip object at 0x7f8847461cc8>\n",
            "batch i:26, episode_len:35\n",
            "kl:0.02055,lr_multiplier:0.444,loss:4.3843865394592285,entropy:3.843871831893921,explained_var_old:0.232,explained_var_new:0.393\n",
            "Time elapsed: 71 seconds\t Total time elapsed: 1665 seconds\n",
            "<zip object at 0x7f8845ecf108>\n",
            "batch i:27, episode_len:23\n",
            "kl:0.03014,lr_multiplier:0.444,loss:4.394103527069092,entropy:3.8015296459198,explained_var_old:0.279,explained_var_new:0.391\n",
            "Time elapsed: 54 seconds\t Total time elapsed: 1720 seconds\n",
            "<zip object at 0x7f88465def88>\n",
            "batch i:28, episode_len:34\n",
            "kl:0.02449,lr_multiplier:0.444,loss:4.40373420715332,entropy:3.810202121734619,explained_var_old:0.272,explained_var_new:0.388\n",
            "Time elapsed: 70 seconds\t Total time elapsed: 1790 seconds\n",
            "<zip object at 0x7f884757c3c8>\n",
            "batch i:29, episode_len:27\n",
            "kl:0.01812,lr_multiplier:0.444,loss:4.3799567222595215,entropy:3.8206260204315186,explained_var_old:0.281,explained_var_new:0.410\n",
            "Time elapsed: 59 seconds\t Total time elapsed: 1850 seconds\n",
            "<zip object at 0x7f8845ecfec8>\n",
            "batch i:30, episode_len:31\n",
            "kl:0.02082,lr_multiplier:0.444,loss:4.398670673370361,entropy:3.7858457565307617,explained_var_old:0.260,explained_var_new:0.385\n",
            "Time elapsed: 64 seconds\t Total time elapsed: 1914 seconds\n",
            "<zip object at 0x7f884673a788>\n",
            "batch i:31, episode_len:24\n",
            "kl:0.02662,lr_multiplier:0.444,loss:4.285341262817383,entropy:3.718809127807617,explained_var_old:0.334,explained_var_new:0.453\n",
            "Time elapsed: 55 seconds\t Total time elapsed: 1971 seconds\n",
            "<zip object at 0x7f8846c64808>\n",
            "batch i:32, episode_len:26\n",
            "kl:0.03232,lr_multiplier:0.444,loss:4.344764709472656,entropy:3.6834943294525146,explained_var_old:0.281,explained_var_new:0.387\n",
            "Time elapsed: 58 seconds\t Total time elapsed: 2028 seconds\n",
            "<zip object at 0x7f88463c58c8>\n",
            "batch i:33, episode_len:23\n",
            "kl:0.03335,lr_multiplier:0.444,loss:4.285370826721191,entropy:3.785292387008667,explained_var_old:0.315,explained_var_new:0.439\n",
            "Time elapsed: 56 seconds\t Total time elapsed: 2086 seconds\n",
            "<zip object at 0x7f8846c64e48>\n",
            "batch i:34, episode_len:30\n",
            "kl:0.01747,lr_multiplier:0.444,loss:4.351954936981201,entropy:3.7563226222991943,explained_var_old:0.247,explained_var_new:0.382\n",
            "Time elapsed: 64 seconds\t Total time elapsed: 2150 seconds\n",
            "<zip object at 0x7f8846734cc8>\n",
            "batch i:35, episode_len:37\n",
            "kl:0.01899,lr_multiplier:0.444,loss:4.318478107452393,entropy:3.729982852935791,explained_var_old:0.285,explained_var_new:0.423\n",
            "Time elapsed: 74 seconds\t Total time elapsed: 2224 seconds\n",
            "<zip object at 0x7f884c608048>\n",
            "batch i:36, episode_len:16\n",
            "kl:0.02072,lr_multiplier:0.444,loss:4.20513916015625,entropy:3.717580795288086,explained_var_old:0.381,explained_var_new:0.512\n",
            "Time elapsed: 44 seconds\t Total time elapsed: 2269 seconds\n",
            "<zip object at 0x7f8846577c08>\n",
            "batch i:37, episode_len:17\n",
            "kl:0.01546,lr_multiplier:0.444,loss:4.264920711517334,entropy:3.691373109817505,explained_var_old:0.379,explained_var_new:0.477\n",
            "Time elapsed: 47 seconds\t Total time elapsed: 2316 seconds\n",
            "<zip object at 0x7f8846577cc8>\n",
            "batch i:38, episode_len:29\n",
            "kl:0.02903,lr_multiplier:0.444,loss:4.161623477935791,entropy:3.6910548210144043,explained_var_old:0.408,explained_var_new:0.536\n",
            "Time elapsed: 64 seconds\t Total time elapsed: 2380 seconds\n",
            "<zip object at 0x7f8881310488>\n",
            "batch i:39, episode_len:19\n",
            "kl:0.02329,lr_multiplier:0.444,loss:4.150118827819824,entropy:3.7111868858337402,explained_var_old:0.411,explained_var_new:0.550\n",
            "Time elapsed: 50 seconds\t Total time elapsed: 2431 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8jOSn_Uhhnxh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "1373/30"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c9-N6TzS2piZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}